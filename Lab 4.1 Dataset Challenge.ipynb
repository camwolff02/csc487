{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec77cf87-1fc6-400d-9078-0eecb604d7a4",
   "metadata": {},
   "source": [
    "# Lab 4.1 Dataset Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d16067b-edf5-4689-9a35-532088be7bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 12:10:34.017778: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-03 12:10:34.025661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738613434.034299  261257 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738613434.036848  261257 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 12:10:34.047188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "from IPython.display import Image \n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.regression import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6f427b-3c38-49bd-8c4a-d63dfff87d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7f2488fa5af5e073\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7f2488fa5af5e073\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard for logging on port 6006\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --bind-all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f42b0-50fb-42d3-97c8-24a2830e264c",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e044c92b-bc24-4fa5-8775-479b6d68b55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>obj_ID</th><th>alpha</th><th>delta</th><th>u</th><th>g</th><th>r</th><th>i</th><th>z</th><th>run_ID</th><th>rerun_ID</th><th>cam_col</th><th>field_ID</th><th>spec_obj_ID</th><th>class</th><th>redshift</th><th>plate</th><th>MJD</th><th>fiber_ID</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1.2377e18</td><td>135.689107</td><td>32.494632</td><td>23.87882</td><td>22.2753</td><td>20.39501</td><td>19.16573</td><td>18.79371</td><td>3606</td><td>301</td><td>2</td><td>79</td><td>6.5438e18</td><td>&quot;GALAXY&quot;</td><td>0.6347936</td><td>5812</td><td>56354</td><td>171</td></tr><tr><td>1.2377e18</td><td>144.826101</td><td>31.274185</td><td>24.77759</td><td>22.83188</td><td>22.58444</td><td>21.16812</td><td>21.61427</td><td>4518</td><td>301</td><td>5</td><td>119</td><td>1.1760e19</td><td>&quot;GALAXY&quot;</td><td>0.779136</td><td>10445</td><td>58158</td><td>427</td></tr><tr><td>1.2377e18</td><td>142.18879</td><td>35.582444</td><td>25.26307</td><td>22.66389</td><td>20.60976</td><td>19.34857</td><td>18.94827</td><td>3606</td><td>301</td><td>2</td><td>120</td><td>5.1522e18</td><td>&quot;GALAXY&quot;</td><td>0.6441945</td><td>4576</td><td>55592</td><td>299</td></tr><tr><td>1.2377e18</td><td>338.741038</td><td>-0.402828</td><td>22.13682</td><td>23.77656</td><td>21.61162</td><td>20.50454</td><td>19.2501</td><td>4192</td><td>301</td><td>3</td><td>214</td><td>1.0301e19</td><td>&quot;GALAXY&quot;</td><td>0.9323456</td><td>9149</td><td>58039</td><td>775</td></tr><tr><td>1.2377e18</td><td>345.282593</td><td>21.183866</td><td>19.43718</td><td>17.58028</td><td>16.49747</td><td>15.97711</td><td>15.54461</td><td>8102</td><td>301</td><td>3</td><td>137</td><td>6.8919e18</td><td>&quot;GALAXY&quot;</td><td>0.1161227</td><td>6121</td><td>56187</td><td>842</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 18)\n",
       "┌───────────┬────────────┬───────────┬──────────┬───┬───────────┬───────┬───────┬──────────┐\n",
       "│ obj_ID    ┆ alpha      ┆ delta     ┆ u        ┆ … ┆ redshift  ┆ plate ┆ MJD   ┆ fiber_ID │\n",
       "│ ---       ┆ ---        ┆ ---       ┆ ---      ┆   ┆ ---       ┆ ---   ┆ ---   ┆ ---      │\n",
       "│ f64       ┆ f64        ┆ f64       ┆ f64      ┆   ┆ f64       ┆ i64   ┆ i64   ┆ i64      │\n",
       "╞═══════════╪════════════╪═══════════╪══════════╪═══╪═══════════╪═══════╪═══════╪══════════╡\n",
       "│ 1.2377e18 ┆ 135.689107 ┆ 32.494632 ┆ 23.87882 ┆ … ┆ 0.6347936 ┆ 5812  ┆ 56354 ┆ 171      │\n",
       "│ 1.2377e18 ┆ 144.826101 ┆ 31.274185 ┆ 24.77759 ┆ … ┆ 0.779136  ┆ 10445 ┆ 58158 ┆ 427      │\n",
       "│ 1.2377e18 ┆ 142.18879  ┆ 35.582444 ┆ 25.26307 ┆ … ┆ 0.6441945 ┆ 4576  ┆ 55592 ┆ 299      │\n",
       "│ 1.2377e18 ┆ 338.741038 ┆ -0.402828 ┆ 22.13682 ┆ … ┆ 0.9323456 ┆ 9149  ┆ 58039 ┆ 775      │\n",
       "│ 1.2377e18 ┆ 345.282593 ┆ 21.183866 ┆ 19.43718 ┆ … ┆ 0.1161227 ┆ 6121  ┆ 56187 ┆ 842      │\n",
       "└───────────┴────────────┴───────────┴──────────┴───┴───────────┴───────┴───────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fedesoriano/stellar-classification-dataset-sdss17\")\n",
    "df = pl.read_csv(os.path.join(path, \"star_classification.csv\"), infer_schema_length=10000)\n",
    "model_path = 'models'\n",
    "train_path = 'runs'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583234b-2a92-4386-a68d-d6e0ea5834b5",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dccf074-9283-47ec-9215-e74de88ab414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state for reproducibility\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccee3453-44a9-4648-a0b2-681492bed548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "test_size = 0.10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1520e09-6392-4324-8fe9-65e990de7fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>alpha</th><th>delta</th><th>u</th><th>g</th><th>r</th><th>i</th><th>z</th><th>redshift</th></tr><tr><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0.376905</td><td>0.503802</td><td>0.999113</td><td>0.99907</td><td>0.535344</td><td>0.427665</td><td>0.998944</td><td>0.091831</td></tr><tr><td>0.402286</td><td>0.491812</td><td>0.999202</td><td>0.999126</td><td>0.646203</td><td>0.515986</td><td>0.999225</td><td>0.112389</td></tr><tr><td>0.39496</td><td>0.534139</td><td>0.999251</td><td>0.999109</td><td>0.546218</td><td>0.435729</td><td>0.998959</td><td>0.09317</td></tr><tr><td>0.940947</td><td>0.1806</td><td>0.998939</td><td>0.99922</td><td>0.596946</td><td>0.486717</td><td>0.99899</td><td>0.13421</td></tr><tr><td>0.959118</td><td>0.392679</td><td>0.99867</td><td>0.998602</td><td>0.337999</td><td>0.287021</td><td>0.99862</td><td>0.017959</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ alpha    ┆ delta    ┆ u        ┆ g        ┆ r        ┆ i        ┆ z        ┆ redshift │\n",
       "│ ---      ┆ ---      ┆ ---      ┆ ---      ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f32      ┆ f32      ┆ f32      ┆ f32      ┆ f32      ┆ f32      ┆ f32      ┆ f32      │\n",
       "╞══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0.376905 ┆ 0.503802 ┆ 0.999113 ┆ 0.99907  ┆ 0.535344 ┆ 0.427665 ┆ 0.998944 ┆ 0.091831 │\n",
       "│ 0.402286 ┆ 0.491812 ┆ 0.999202 ┆ 0.999126 ┆ 0.646203 ┆ 0.515986 ┆ 0.999225 ┆ 0.112389 │\n",
       "│ 0.39496  ┆ 0.534139 ┆ 0.999251 ┆ 0.999109 ┆ 0.546218 ┆ 0.435729 ┆ 0.998959 ┆ 0.09317  │\n",
       "│ 0.940947 ┆ 0.1806   ┆ 0.998939 ┆ 0.99922  ┆ 0.596946 ┆ 0.486717 ┆ 0.99899  ┆ 0.13421  │\n",
       "│ 0.959118 ┆ 0.392679 ┆ 0.99867  ┆ 0.998602 ┆ 0.337999 ┆ 0.287021 ┆ 0.99862  ┆ 0.017959 │\n",
       "└──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate data and labels, dropping irrelevant/ID features to predicting class\n",
    "not_features = [\"class\", \"obj_ID\", \"run_ID\", \"rerun_ID\", \"cam_col\", \"field_ID\", \"spec_obj_ID\", \"plate\", \"MJD\", \"fiber_ID\"]\n",
    "X_df, y_df_str = df.drop(not_features), df[\"class\"]\n",
    "\n",
    "# Convert labels to enum for numeric computation\n",
    "labels = pl.Enum([\"GALAXY\", \"STAR\", \"QSO\"])\n",
    "y_df = pl.Series(y_df_str, dtype=labels)\n",
    "\n",
    "# Min max normalization\n",
    "X_df = X_df.select((pl.all() - pl.all().min()) / (pl.all().max() - pl.all().min()))\n",
    "X_df = X_df.cast(pl.Float32)  # Convert data to 32-bit precision\n",
    "\n",
    "# Create train-test-validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y_df.to_physical(), test_size=test_size, random_state=random_state)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "    X_train, y_train, test_size=test_size/(1 - test_size), random_state=random_state)\n",
    "\n",
    "# Create a dataloader \n",
    "X_train_tensor, y_train_tensor = X_train.to_torch().to(device), y_train.to_torch().to(device)\n",
    "X_test_tensor, y_test_tensor = X_test.to_torch().to(device), y_test.to_torch().to(device)\n",
    "X_validate_tensor, y_validate_tensor = X_validate.to_torch().to(device), y_validate.to_torch().to(device)\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_tensor, y_train_tensor),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True)\n",
    "\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14421f81-97dd-4f54-a179-cabff2a6dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (3,)\n",
       " Series: 'class' [enum]\n",
       " [\n",
       " \t\"GALAXY\"\n",
       " \t\"STAR\"\n",
       " \t\"QSO\"\n",
       " ],\n",
       " shape: (3,)\n",
       " Series: 'class' [u32]\n",
       " [\n",
       " \t59445\n",
       " \t18961\n",
       " \t21594\n",
       " ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.unique(), y_df.unique_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94586998-30c1-4545-899e-fb4952bd5e01",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe59d39f-60a4-4ffa-bbae-b17661c7c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    model.eval()  # set model to evaluation mode, disable certain features\n",
    "    with torch.no_grad():  # disable gradient calculations\n",
    "        z = model(X)\n",
    "        preds = torch.argmax(z, dim=-1)\n",
    "        return (preds == y).float().mean().item()\n",
    "\n",
    "def calc_error(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    model.eval()  # set model to evaluation mode, disable certain features\n",
    "    with torch.no_grad():  # disable gradient calculations\n",
    "        z = model(X)\n",
    "        preds = torch.argmax(z, dim=-1)\n",
    "        mse = MeanSquaredError().to(device)\n",
    "        return mse(preds, y).item()\n",
    "\n",
    "def eval_model(\n",
    "    model: nn.Module, \n",
    "    model_id: int,\n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor, \n",
    "    y_test: torch.Tensor,\n",
    ") -> None:\n",
    "    print(\"+-------------+---------------------+\")\n",
    "    print(f\"| model       | {model_id} |\")\n",
    "    print(\"+-------------+---------------------+\")\n",
    "    print(f\"| train acc   | {calc_accuracy(model, X_train, y_train):.17f} |\")\n",
    "    print(f\"| test acc    | {calc_accuracy(model, X_test, y_test):.17f} |\")\n",
    "    print(f\"| train error | {calc_error(model, X_train, y_train):.17f} |\")\n",
    "    print(f\"| test error  | {calc_error(model, X_test, y_test):.17f} |\")\n",
    "    print(\"+-------------+---------------------+\")\n",
    "    \n",
    "def train_model(\n",
    "    model: nn.Module, \n",
    "    opt: torch.optim.Optimizer,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
    "    epochs: int = 100,\n",
    "    plot_every: int = 1,\n",
    ") -> int:\n",
    "    unique_model_no = abs(hash((str(model), str(opt), str(loss_fn))))\n",
    "    with SummaryWriter(os.path.join(train_path, f\"model_{unique_model_no}\")) as writer:\n",
    "        model.train()  # set model to training mode\n",
    "\n",
    "        n_total_steps = len(dataloader)\n",
    "\n",
    "        # create metrics to display while training\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for (X_batch, y_batch) in dataloader:\n",
    "                opt.zero_grad()\n",
    "    \n",
    "                z = model(X_batch)\n",
    "                loss = loss_fn(z,y_batch)\n",
    "    \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "    \n",
    "                # calculate metrics\n",
    "                running_loss = loss.item()\n",
    "                predicted = torch.argmax(z, dim=-1)\n",
    "                running_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "                # logging and saving model\n",
    "                if (epoch+1) % plot_every == 0:\n",
    "                    step = epoch * n_total_steps + epoch \n",
    "                    \n",
    "                    path = os.path.join(model_path, f\"model_{unique_model_no}\", f\"{step}\")\n",
    "                    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                    torch.save(model.state_dict(), path)\n",
    "                    \n",
    "                    writer.add_scalar('training loss', running_loss / plot_every, step)\n",
    "                    writer.add_scalar('accuracy', running_correct / plot_every, step)\n",
    "                    \n",
    "                    running_loss = 0.0\n",
    "                    running_correct = 0\n",
    "                    \n",
    "    return unique_model_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b52265-b915-4d32-8d8e-6db47c2c86e1",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333bdc4-f581-4776-ad2c-bd6cc0783091",
   "metadata": {},
   "source": [
    "Training a simple linear model on data, using standard hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f37f3e-6ed5-478b-b7df-76fdb8cea86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "| model       | 2711907495640221595 |\n",
      "+-------------+---------------------+\n",
      "| train acc   | 0.95538747310638428 |\n",
      "| test acc    | 0.95419996976852417 |\n",
      "| train error | 0.13881249725818634 |\n",
      "| test error  | 0.13879999518394470 |\n",
      "+-------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "simple_nn = nn.Sequential(\n",
    "    nn.Linear(8, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 3)\n",
    ").to(device)\n",
    "\n",
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(simple_nn.parameters(), lr=lr)\n",
    "\n",
    "model_id_0 = train_model(simple_nn, opt, train_loader)\n",
    "\n",
    "os.system('spd-say \"training over\"')  # Text-to-speech to tell me when training is over so I can clean my room lol\n",
    "eval_model(simple_nn, model_id_0, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef6a36-9f85-4e23-ad1a-3c0b2a4abd7b",
   "metadata": {},
   "source": [
    "Pretty good, but let's try and do better with regularization and other techniques. Not sure if we'll get there but we'll see.\n",
    "\n",
    "Notes for interpreting model training performance:\n",
    "- Accuracy: Higher the better\n",
    "- Loss: Want smooth inverse curve converging at low loss\n",
    "- Underfitting: high bias, low variance, high train error, high test error\n",
    "- Overfitting: low bias, high variance, low train error, high test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd56dc44-8334-43c7-8ebc-e86ec5bc309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "| model       | 8331765224541759696 |\n",
      "+-------------+---------------------+\n",
      "| train acc   | 0.94991248846054077 |\n",
      "| test acc    | 0.94559997320175171 |\n",
      "| train error | 0.14155000448226929 |\n",
      "| test error  | 0.14650000631809235 |\n",
      "+-------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Adding momentum, decrease learning rate\n",
    "nn_1 = nn.Sequential(\n",
    "    nn.Linear(8, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 3)\n",
    ").to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "mu = 0.9\n",
    "opt = torch.optim.SGD(nn_1.parameters(), lr=lr, momentum=mu)\n",
    "model_id_1 = train_model(nn_1, opt, train_loader)\n",
    "\n",
    "os.system('spd-say \"training over\"')  # Text-to-speech to tell me when training is over so I can clean my room lol\n",
    "eval_model(nn_1, model_id_1, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e507e-24e3-40bc-a8a6-3854b420c192",
   "metadata": {},
   "source": [
    "This model converges much faster, and in training looks like loss is higher. But on evaluation, loss accuracy is lower than the original :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad6f142-0d43-406a-a2cf-d91cbed19f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "| model       | 3467952820851895034 |\n",
      "+-------------+---------------------+\n",
      "| train acc   | 0.96274995803833008 |\n",
      "| test acc    | 0.96319997310638428 |\n",
      "| train error | 0.12582500278949738 |\n",
      "| test error  | 0.12409999966621399 |\n",
      "+-------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Increasing epochs, adding weight decay, switch activation function to swish\n",
    "nn_2 = nn.Sequential(\n",
    "    nn.Linear(8, 6),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(6, 3)\n",
    ").to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 300  # aim for much higher to find best model\n",
    "mu = 0.9\n",
    "l2_reg = 1e-4\n",
    "opt = torch.optim.SGD(nn_2.parameters(), lr=lr, momentum=mu, weight_decay=l2_reg)\n",
    "model_id_2 = train_model(nn_2, opt, train_loader, epochs=epochs)\n",
    "\n",
    "os.system('spd-say \"training over\"')  # Text-to-speech to tell me when training is over so I can clean my room lol\n",
    "eval_model(nn_2, model_id_2, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5389181-39fd-40b5-8cc5-d908de359a50",
   "metadata": {},
   "source": [
    "This model, by training longer and adding more regularization, finally outperforms the other two! Let's see the training graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "666b576e-67a6-45c1-b8c5-ef4851d6eb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"graphs/lab_4.1_key.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=os.path.join(\"graphs\", \"lab_4.1_key.png\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36855f35-c4d3-4da4-8232-43567fbaea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"graphs/lab_4.1_loss.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=os.path.join(\"graphs\", \"lab_4.1_loss.png\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3963470-cb2c-4919-b64b-e17e74ec5ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"graphs/lab_4.1_accuracy.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=os.path.join(\"graphs\", \"lab_4.1_accuracy.png\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee5934-2a7c-4fd7-8773-0898ea4bc96f",
   "metadata": {},
   "source": [
    "And a final validation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aeb4fea-812f-4f6c-a277-3e8981b05881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535999894142151\n",
      "0.9506999850273132\n",
      "0.9624999761581421\n"
     ]
    }
   ],
   "source": [
    "print(calc_accuracy(simple_nn, X_validate_tensor, y_validate_tensor))\n",
    "print(calc_accuracy(nn_1, X_validate_tensor, y_validate_tensor))\n",
    "print(calc_accuracy(nn_2, X_validate_tensor, y_validate_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050448f4-e7af-4b63-bf81-e903545ce909",
   "metadata": {},
   "source": [
    "## Success! regularization increased accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
