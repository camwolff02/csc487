{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 8.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will explore different methods of text tokenization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download the text of Shakespeare's sonnets and read it in as one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sonnets.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
    "text = (open(\"sonnets.txt\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to thine own bright eyes,\n",
      " feed'st thy light's flame with self-substantial fuel,\n",
      " making a famine where abundance lies,\n",
      " thy self thy foe, to thy sweet self too cruel:\n",
      " thou that art now the world's fresh ornament,\n",
      " and only herald to the gaudy spring,\n",
      " within thine own bud buriest thy content,\n",
      " and tender churl mak'st waste in niggarding:\n",
      "   pity the world, or else this glutton be,\n",
      "   to eat the world's due, by the grave and thee.\n",
      "\n",
      " ii\n",
      "\n",
      " when forty winters shall besiege thy brow,\n",
      " and dig deep trenches in thy beauty's field,\n",
      " thy youth's proud livery so gazed on now,\n",
      " will be a tatter'd weed of small worth held:\n",
      " then being asked, where all thy beauty lies,\n",
      " where all the treasure of thy lusty days;\n",
      " to say, within thine own deep sunken eyes,\n",
      " were an all-eating shame, and thriftless praise.\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Prepare a vocabulary of the unique words in the dataset.  (For simplicity's sake you can leave the punctuation in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4942"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(text.split(' '))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now you will make a Dataset subclass that can return sequences of tokens, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, text, seq_len=100):\n",
    "        self.seq_len = seq_len\n",
    "        # add code to compute the vocabulary (copied from exercise 1)\n",
    "        words = text.split(' ')\n",
    "        self.vocab = [word for word in set(words)]\n",
    "        # add code to convert the text to a sequence of word indices\n",
    "        encode = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.sequence = [encode[word] for word in words if word in encode]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the number of possible sub-sequences \n",
    "        return len(self.sequence) * self.seq_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return the sequence of token indices starting at i and the index of token i+seq_len as the label\n",
    "        return self.sequence[i:i+self.seq_len], self.sequence[i+self.seq_len]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        # convert a sequence of tokens back into a string \n",
    "        return ' '.join([self.vocab[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Verify that your class can successfully encode and decode sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_encode = '''when forty winters shall besiege thy brow,\n",
    " and dig deep trenches in thy beauty's field,\n",
    " thy youth's proud livery so gazed on now,\n",
    " will be a tatter'd weed of small worth held:\n",
    " then being asked, where all thy beauty lies,\n",
    " where all the treasure of thy lusty days;\n",
    " to say, within thine own deep sunken eyes,\n",
    " were an all-eating shame, and thriftless praise.'''\n",
    "\n",
    "seq_len = 4\n",
    "dataset = WordDataset(text_to_encode, seq_len=seq_len)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([29, 34, 48, 12], 47)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when forty winters shall besiege thy brow,\\n and dig deep trenches in thy beauty's field,\\n thy youth's proud livery so gazed on now,\\n will be a tatter'd weed of small worth held:\\n then being asked, where all thy beauty lies,\\n where all the treasure of thy lusty days;\\n to say, within thine own deep sunken eyes,\\n were an all-eating shame, and thriftless praise.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(dataset.sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do the exercise again, but this time at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "    def __init__(self, text, seq_len=100):\n",
    "        self.seq_len = seq_len\n",
    "        # add code to compute the vocabulary of unique characters\n",
    "        letters = [letter for letter in text]\n",
    "        self.vocab = [letter for letter in set(letters)]\n",
    "        # add code to convert the text to a sequence of character indices\n",
    "        encode = {letter: idx for idx, letter in enumerate(self.vocab)}\n",
    "        self.sequence = [encode[letter] for letter in letters if letter in encode]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return the number of possible sub-sequences\n",
    "        return len(self.sequence) * self.seq_len\n",
    "    def __getitem__(self,i):\n",
    "        # return the sequence of token indices starting at i and the index of token i+1 as the label\n",
    "        return self.sequence[i:i+self.seq_len], self.sequence[i+self.seq_len]\n",
    "\n",
    "    def decode(self,tokens):\n",
    "        # convert a sequence of tokens back into a string\n",
    "        return ''.join([self.vocab[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_encode = '''when forty winters shall besiege thy brow,\n",
    " and dig deep trenches in thy beauty's field,\n",
    " thy youth's proud livery so gazed on now,\n",
    " will be a tatter'd weed of small worth held:\n",
    " then being asked, where all thy beauty lies,\n",
    " where all the treasure of thy lusty days;\n",
    " to say, within thine own deep sunken eyes,\n",
    " were an all-eating shame, and thriftless praise.'''\n",
    "\n",
    "dataset2 = CharacterDataset(text_to_encode, seq_len=seq_len)\n",
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10, 30, 8, 27], 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when forty winters shall besiege thy brow,\\n and dig deep trenches in thy beauty's field,\\n thy youth's proud livery so gazed on now,\\n will be a tatter'd weed of small worth held:\\n then being asked, where all thy beauty lies,\\n where all the treasure of thy lusty days;\\n to say, within thine own deep sunken eyes,\\n were an all-eating shame, and thriftless praise.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.decode(dataset2.sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compare the number of sequences for each tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sequence length: 4\n",
      "Word tokenization: 252\n",
      "Letter tokenization: 1440\n"
     ]
    }
   ],
   "source": [
    "print(f'Using sequence length: {seq_len}')\n",
    "print(f'Word tokenization: {len(dataset)}')\n",
    "print(f'Letter tokenization: {len(dataset2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter tokenization is much larget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Optional: implement the byte pair encoding algorithm to make a Dataset class that uses word parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm sorry Dr. Ventura it's been a very busy quarter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
