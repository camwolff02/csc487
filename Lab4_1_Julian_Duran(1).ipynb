{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gVulGUPHVWx7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "!pip install ucimlrepo --quiet\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty8BzajVYkbH"
   },
   "source": [
    "Import the wine quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEcPwgj0XwfC",
    "outputId": "706a9e00-a926-4c89-ea3c-b37391f7d03d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n",
      "       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol'],\n",
      "      dtype='object')\n",
      "Index(['quality'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "  # fetch dataset\n",
    "wine = fetch_ucirepo(id=186)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = wine.data.features\n",
    "y = wine.data.targets\n",
    "\n",
    "# metadata\n",
    "print(X.columns)\n",
    "\n",
    "# variable information\n",
    "print(y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InbYqMOXYq9p"
   },
   "source": [
    "Remove any missing values from features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8jKH5EPIYcAD"
   },
   "outputs": [],
   "source": [
    "bad = X.isna().any(axis=1)\n",
    "X = X[~bad]\n",
    "y = y[~bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3cEyBMF5Y5KF"
   },
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b13qV0eqnGwv"
   },
   "source": [
    "Normalize y's values -- the minimum class for y is 3, and the maximum 9, so we will shift it downwards to min 0 and max 6, so that we only have the network predicting 7 classes instead of 10, which would be necessary if this step wasn't taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XSQtosGim1Hx"
   },
   "outputs": [],
   "source": [
    "y = y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaLc5QP4ZQ7T"
   },
   "source": [
    "Turn X and y arrays into torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sXIsuNo5ZFU7"
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE2noDD3Zmq9"
   },
   "source": [
    "Divide data into train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8MNGZ4Iea9SN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k_onqPsZyF3"
   },
   "source": [
    "Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N1Ivp-7Ga_GJ"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "nn_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(11, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJTsLd2vbjVh"
   },
   "source": [
    "Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PMVmEMySgkBE"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nn_model.to(device)\n",
    "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j8DJukt8blW3"
   },
   "outputs": [],
   "source": [
    "# optimizer and loss for basic NN\n",
    "lr = 1e-4\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(nn_model.parameters(), lr=lr)\n",
    "\n",
    "# accuracy scoring function\n",
    "def accuracy(model, X, y):\n",
    "  z = model(X)\n",
    "  pred = torch.argmax(z, dim=1)\n",
    "  return (pred==y).float().mean().item()\n",
    "\n",
    "def train(model, loss_fn, opt):\n",
    "  # training loop\n",
    "  epochs = 2000\n",
    "  for epoch in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    z = model(X_train)\n",
    "    loss = loss_fn(z, y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 100 == 0:\n",
    "      print(f\"epoch {epoch} loss = {loss:.4f} accuracy = {accuracy(model, X_test, y_test):.4f}\")\n",
    "    else:\n",
    "      #print(f\"epoch {epoch} loss = {loss}\")\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcd3PAvJCqZT"
   },
   "source": [
    "Evaluate Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtBrwGDo1SbO",
    "outputId": "ce02f960-9902-4bb2-c9a1-6816101831ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss = 2.5665 accuracy = 0.0338\n",
      "epoch 100 loss = 1.5043 accuracy = 0.4262\n",
      "epoch 200 loss = 1.4283 accuracy = 0.4215\n",
      "epoch 300 loss = 1.4033 accuracy = 0.4292\n",
      "epoch 400 loss = 1.3909 accuracy = 0.4338\n",
      "epoch 500 loss = 1.3832 accuracy = 0.4323\n",
      "epoch 600 loss = 1.3776 accuracy = 0.4354\n",
      "epoch 700 loss = 1.3732 accuracy = 0.4431\n",
      "epoch 800 loss = 1.3695 accuracy = 0.4446\n",
      "epoch 900 loss = 1.3662 accuracy = 0.4462\n",
      "epoch 1000 loss = 1.3633 accuracy = 0.4508\n",
      "epoch 1100 loss = 1.3607 accuracy = 0.4508\n",
      "epoch 1200 loss = 1.3582 accuracy = 0.4508\n",
      "epoch 1300 loss = 1.3559 accuracy = 0.4585\n",
      "epoch 1400 loss = 1.3538 accuracy = 0.4631\n",
      "epoch 1500 loss = 1.3518 accuracy = 0.4631\n",
      "epoch 1600 loss = 1.3499 accuracy = 0.4615\n",
      "epoch 1700 loss = 1.3480 accuracy = 0.4631\n",
      "epoch 1800 loss = 1.3463 accuracy = 0.4646\n",
      "epoch 1900 loss = 1.3446 accuracy = 0.4631\n",
      "=== Train dataset accuracy: 0.4283 ===\n",
      "=== Test dataset accuracy:  0.4646 ===\n"
     ]
    }
   ],
   "source": [
    "train(nn_model, loss_fn, opt)\n",
    "print(f\"=== Train dataset accuracy: {accuracy(nn_model, X_train, y_train):.4f} ===\")\n",
    "print(f\"=== Test dataset accuracy:  {accuracy(nn_model, X_test, y_test):.4f} ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdHu9lLCoPBC"
   },
   "source": [
    "After training the neural network using only basic gradient descent, we are seeing:  \n",
    "training dataset accuracy of 0.4330  \n",
    "test dataset accuracy of     0.4692  \n",
    "  \n",
    "Now we will apply some techniques learned in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G6ke_bOn0kUa"
   },
   "outputs": [],
   "source": [
    "# new nn to avoid inheriting params from previous one\n",
    "# also added an additional hidden layer\n",
    "nn_model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "nn_model2.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM3A0-yV6GJ5"
   },
   "source": [
    "Data manipulation to facilitate better training outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OEVkXoXW6F9F"
   },
   "outputs": [],
   "source": [
    "X2 = X - X.mean(dim=0)\n",
    "X2 /= X2.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLIjSluIAdyr"
   },
   "source": [
    "PCA transformation and new train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w2sOd1pT_cvm"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8, whiten=True)\n",
    "pca.fit(X_train.cpu().numpy())\n",
    "\n",
    "X2 = pca.transform(X2.cpu().numpy())\n",
    "\n",
    "y_norm = (y + 3) / 10.0  # scale to between 0 and 1\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y_norm.cpu().numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the NumPy arrays back to PyTorch tensors and move to the device\n",
    "X2_train, y2_train, X2_test, y2_test = \\\n",
    "torch.tensor(X2_train).to(device), \\\n",
    "torch.tensor(y2_train).to(device), \\\n",
    "torch.tensor(X2_test).to(device), \\\n",
    "torch.tensor(y2_test).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipTd9Sdl3Tbv"
   },
   "source": [
    "SGD optimization with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KZU14azJ3I4H"
   },
   "outputs": [],
   "source": [
    "lr2 = 1e-4\n",
    "opt2 = torch.optim.SGD(nn_model2.parameters(), lr=lr2, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiudThwn3dPR"
   },
   "source": [
    "Batch data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7r_d1tOV3jUl"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# training data loader\n",
    "train_ds = TensorDataset(X2_train, y2_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# testing data loader\n",
    "test_ds = TensorDataset(X2_test, y2_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "x8pxV6dt6omS"
   },
   "outputs": [],
   "source": [
    "loss_fn2 = torch.nn.MSELoss()\n",
    "\n",
    "# new accuracy scoring function\n",
    "def accuracy2(model2, data_loader):\n",
    "  correct_preds, total_preds = 0, 0\n",
    "\n",
    "  for x, y in data_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    z = model2(x)\n",
    "    correct_preds += (torch.round(z, decimals=1) == torch.round(y, decimals=1)).sum().item()\n",
    "    total_preds += y.size(0)\n",
    "\n",
    "  accuracy = correct_preds / total_preds\n",
    "  return accuracy\n",
    "\n",
    "def train2(model2, loss_fn2, opt2):\n",
    "  # new training loop\n",
    "  epochs = 2000\n",
    "  for epoch in range(epochs):\n",
    "    for x, y in train_dl:\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      opt2.zero_grad()\n",
    "      z = model2(x)\n",
    "      loss = loss_fn2(z, y)\n",
    "      loss.backward()\n",
    "      opt2.step()\n",
    "    if epoch % 100 == 0:\n",
    "      print(f\"epoch {epoch} loss = {loss:.4f} accuracy = {accuracy2(nn_model2, test_dl):.4f}\")\n",
    "    else:\n",
    "      #print(f\"epoch {epoch} loss = {loss}\")\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "YcG_L_Z37kMA",
    "outputId": "ed11ec3f-62e7-4990-bf23-fcb5ac708204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss = 0.0184 accuracy = 9.8308\n"
     ]
    }
   ],
   "source": [
    "train2(nn_model2, loss_fn2, opt2)\n",
    "print(f\"=== Train dataset accuracy: {accuracy2(nn_model2, train_dl):.4f} ===\")\n",
    "print(f\"=== Test dataset accuracy:  {accuracy2(nn_model2, test_dl):.4f} ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcQutxaQTF0O"
   },
   "source": [
    "We were able to improve the test dataset accuracy by almost 7 percent! Interestingly, the test dataset accuracy is higher than the train dataset accuracy for both models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
