{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU acceleration\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5., requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6., requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 0.20828595757484436\n",
      "epoch 99: loss is 0.13075871765613556\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    if epoch == 0 or epoch == epochs-1:\n",
    "        print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 3.1929023265838623\n",
      "epoch 99999: loss is 0.11726874113082886\n"
     ]
    }
   ],
   "source": [
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 100),\n",
    "    torch.nn.Linear(100, 3),\n",
    ")\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 100_000\n",
    "mlp_opt = torch.optim.SGD(mlp.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mlp_opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    mlp_opt.step() # apply gradients\n",
    "    \n",
    "    if epoch == 0 or epoch == epochs-1:\n",
    "        print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        z = torch.argmax(model(X), dim=1)\n",
    "        return (z == y).float().mean().item()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model accuracy: 0.954954981803894\n",
      "Multi-Layer Perceptron accuracy: 0.9579579830169678\n"
     ]
    }
   ],
   "source": [
    "print(f'Linear Model accuracy: {accuracy(linear_model, X, y)}')\n",
    "print(f'Multi-Layer Perceptron accuracy: {accuracy(mlp, X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wohoo! MLP has a higher accuracy! (even if it's just a little)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
